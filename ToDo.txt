V some kind of normalization to be able to predict both theta and d(theta)/dt ? -> in RL part
V implement PD controller -> in environment
V Remove "prune.remove" in BrainRNN -> only to temporarily fix deepcopy issue
~V Entre temps, regarder l'erreur de dimensions (avec solution temporaire d'au dessus)
    -> Redéfinir variables/specs avec Observations, Action, Params ?
- Regarder pourquoi pbm avec BatchedTensor
    -> Time dimension in GAE: je voulais x [B, F] et j'ai x [B, T, F] (F=Feature, T=time)
        -> next_hidden_states = torch.empty(*x.shape[:-1], self.n_neurons)
V Modifier graph (adj_mat) pour avoir que les neurones de LegNpT1_L par ex.
V Est ce qu'on a le bon nb de neurones ??
    - 93 types du panel C + filtrage sur target == 'LegNpT1_L'
    - 1717 target=='LegNpT1_L'
    - 1108 (target=='LegNpT1_L' & class == 'intrinsic neuron')
    - certains types du Panel C ne sont pas dans target == 'LegNpT1_L', d'autres y sont en plusieurs exemplaires (3 id différents du même type)

V FIX BATCH SIZE IN RUN
V For "by_hand ppo": check dimensions between output/action (in BrainRNN and RL part)
V Modifier PPO pour évaluer de manière séquentielle !!! -> Sample batches of sequences and forward each element os sequence

V before modifying reward: just chack that reward*2 in Wrapper produces good output/works as intended
V Modify reward function to have something ~like deepmimic & help training -> OBTAIN A REFERENCE MOTION: by hand or train ppo_tutorial on colab, save weights and save info of success episode

V modifier PPO.train -> on doit voir trop de fois les mêmes step avec la méthode actuelle

- Modifier les poids task/imitation 

- Ajuster les poids initiaux de BrainRNN en fonction des weights de adj_mat

- Modifier output layer pour avoir (potentiellement) tous les neurones qui puissent arriver aux motorneurons (voir avec Sibo)

- see how to reset hidden states -> ~fixed initialization to have stability in learning ?
- 0 MotroNeuron with target = LegNpT1_L => empty layer at the end of the graph => Error

- Truncated BPTT -> faster run // Already implemented in ¨PPO with sample_mb ?

