V some kind of normalization to be able to predict both theta and d(theta)/dt ? -> in RL part
V implement PD controller -> in environment
V Remove "prune.remove" in BrainRNN -> only to temporarily fix deepcopy issue
~V Entre temps, regarder l'erreur de dimensions (avec solution temporaire d'au dessus)
    -> Redéfinir variables/specs avec Observations, Action, Params ?
- Regarder pourquoi pbm avec BatchedTensor
    -> Time dimension in GAE: je voulais x [B, F] et j'ai x [B, T, F] (F=Feature, T=time)
        -> next_hidden_states = torch.empty(*x.shape[:-1], self.n_neurons)
V Modifier graph (adj_mat) pour avoir que les neurones de LegNpT1_L par ex.
V Est ce qu'on a le bon nb de neurones ??
    - 93 types du panel C + filtrage sur target == 'LegNpT1_L'
    - 1717 target=='LegNpT1_L'
    - 1108 (target=='LegNpT1_L' & class == 'intrinsic neuron')
    - certains types du Panel C ne sont pas dans target == 'LegNpT1_L', d'autres y sont en plusieurs exemplaires (3 id différents du même type)

V FIX BATCH SIZE IN RUN
V For "by_hand ppo": check dimensions between output/action (in BrainRNN and RL part)
V Modifier PPO pour évaluer de manière séquentielle !!! -> Sample batches of sequences and forward each element os sequence

V before modifying reward: just chack that reward*2 in Wrapper produces good output/works as intended
V Modify reward function to have something ~like deepmimic & help training -> OBTAIN A REFERENCE MOTION: by hand or train ppo_tutorial on colab, save weights and save info of success episode

V modifier PPO.train -> on doit voir trop de fois les mêmes step avec la méthode actuelle

~V Truncated BPTT -> faster run // Already implemented in ¨PPO with sample_mb ?
~V Modifier les poids task/imitation 
~V see how to reset hidden states -> ~fixed initialization to have stability in learning ? -> Random seems to perform better

V REVOIR LAYERS DANS BRAINRNN -> recurrent and skip avaient un ==1 au lieu de >0 ET GROS PBM SUR QUELLES CONNECTIONS CHOISIES !!!

X Train sur un mouvement nul (oscillations, jambes tendues) pour voir si le training marche-> Pbm de video qui s'arête...

V Ajuster les poids initiaux de BrainRNN en fonction des weights de adj_mat

V save rewards of training !

V link input feedback to more nodes (random with different nb targeted nodes, and see how well the network performs)

V !!!!!!!!! Modifier output layer: pour le moment c'est que dernière layer -> output, mais last layer = 1 neurone => GIGA perte d'information

V run on Scitas:
    V !!!! See memory issue -> remove video save ??
    V have a .run file (as Sibo sent)
    V possible to put many (python my_script.py arg1   -new line-   python my_script.py arg2, etc) -> run sequentially => not optimal
    V ATTENTION nom de fichier enregistrés ! -> -f argument (suffixe to filename)

V !!!!!!!!!!! REMOVE VIDEO SAVE in next runs -> TAKES AGES !!! -> Done + remove intermediate model save 

V Add loss save

V Decrease sigma after ~1000 steps to train better value network

- End action plots test (4 plots instead of 2 + why torque at 0)

- test 'random_sequential' strategy for coloring (and not have potential information bottleneck due to largest first strategy) BUT still check that the result is ok (colors, etc)

- Modifier reward pour que ce soit indépendant (?) de la taille de l'épisode -> pour le moment, rester debout sans avancer rapporte pas mal de reward
    -> Plutôt early termination sur mouvement bloqué & durée épisode plus grande pour récompenser ceux qui vont loin

- Modifier output layer pour avoir (potentiellement) tous les neurones qui puissent arriver aux motorneurons (voir avec Sibo)

- 0 MotroNeuron with target = LegNpT1_L => empty layer at the end of the graph => Error


Training:
- init random: marche mieux que 0 (avant avoir changé de std)
- std in policy_net:
    - std = 0.1 => keeps doing the same mvt
    - std = 2 or 3 => more mvt exploration, some very good sequences

Rq:
- Leg 1 = claire ; Leg 2 = Foncée